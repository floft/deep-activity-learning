#!/bin/bash
#SBATCH --job-name=DAL
#SBATCH --output=slurm_logs/dal_%j.out
#SBATCH --error=slurm_logs/dal_%j.err
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --nodes=1-1
#SBATCH --gres=gpu:tesla:1
#SBATCH --partition=taylor,cahnrs_gpu,free_gpu,hpc_club,kamiak
#SBATCH --time=1-00:00:00
#SBATCH --mem=20G

# Note: if changing time above, also change on scratch dir command

#
# This script runs three networks at the same time on the same GPU -- folds 0-2.
# You must specify what target you want e.g. --target=hh101 as an argument.
# Then just start one of these jobs for each target.
#

. kamiak_config.sh

#
# ---
#

first_arg=$1
if [[ -z $first_arg ]]; then
    echo "Specify what method to use, e.g. --flat"
    exit 1
else
    echo "Args: $@"
fi

# Allow overriding the dataset zip file with --dataset=... etc.
# Other args are passed directly to the Python program.
program_args=()
for i; do
    name="$(cut -d'=' -f1 <<< "$i")"
    value="$(cut -d'=' -f2 <<< "$i")"

    if [[ "$name" == "--dataset" ]]; then
        compressedDataset="$value"
        echo "Overriding dataset to be: $compressedDataset"
    elif [[ "$name" == "--logdir" ]]; then
        logFolder="$value"
        echo "Overriding logdir to be: $logFolder"
    elif [[ "$name" == "--modeldir" ]]; then
        modelFolder="$value"
        echo "Overriding modeldir to be: $modelFolder"
    else
        program_args+=("$i")
    fi

    shift
done

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
module load cuda/9.0.176 cudnn/7.1.2_cuda9.0 python3/3.5.0
data="$remotedir"

# If an error occurs on one fold, kill the rest so we know it errored
# before we wait hours and have to start the job again
thread_kill() {
    local t

    for t; do
        kill $t &>/dev/null
    done

    wait
}

thread_list=()
handle_error() {
    echo "Error occured -- exiting"

    # However, we're in a subprocess and we can't tell the parent process via
    # any variable to exit 1, thus let's just kill the parent process, which
    # will signal handle_terminate() that then cleans up and exits with 1
    # You'll know it's because of handle_error if you see both the "Error ..."
    # and "Sigterm ..." messages output to the log file. Slurm will report
    # FAILED, which makes it easy to identify jobs with a problem. The top
    # of the .out log file lists the arguments, so re-run the job with those
    # arguments.
    kill $$
}

handle_terminate() {
    echo "Exiting"
    thread_kill "${thread_list[@]}"
    cleanup
    exit 1
}

# Remove workspace at the end, end all remaining processes
cleanup() {
    rmworkspace -a -f --name="$SCRATCHDIR"
}

# Create a scratch workspace
SCRATCHDIR="$(mkworkspace -q -t 1-00:00:00 -b /local)"
trap 'handle_terminate' SIGTERM SIGINT

echo "Scratch space: $SCRATCHDIR"
echo "SLURM_CPUS_PER_TASK: $SLURM_CPUS_PER_TASK"
echo "SLURM_JOB_GPUS: $SLURM_JOB_GPUS"

# This is somewhat sensitive data
chmod 0700 "$SCRATCHDIR"

# Get data
echo "Getting data: started"
cd "$SCRATCHDIR"
echo " - program"
cp -a "$data"/*.py .
echo " - dataset"
cp -a "$data/$compressedDataset" .
unzip "$compressedDataset"
echo " - config file"
mkdir -p preprocessing/al-features/
cp -a "$data/preprocessing/al-features/al.config" preprocessing/al-features/
echo "Getting data: done"

# Install dependencies
echo "Making sure TensorFlow installed: starting"
# I upgraded tensorflow with:
#   module load python3/3.5.0
#   pip install --user --upgrade --no-cache-dir tensorflow-gpu
pip install --user tensorflow-gpu pillow lxml jupyter matplotlib pandas sklearn scipy
echo "Making sure TensorFlow installed: done"

# Train
echo "Training network: started"
mkdir -p "$data/$logFolder/" # log dir, rsync this to view with TensorBoard

python3 "$program" --logdir "$data/$logFolder" --modeldir "$data/$modelFolder" \
    --debug-num=0 --fold=0 --gpu-mem=0.3 "${program_args[@]}" || handle_error &
thread_list+=("$!")

python3 "$program" --logdir "$data/$logFolder" --modeldir "$data/$modelFolder" \
    --debug-num=1 --fold=1 --gpu-mem=0.3 "${program_args[@]}" || handle_error &
thread_list+=("$!")

python3 "$program" --logdir "$data/$logFolder" --modeldir "$data/$modelFolder" \
    --debug-num=2 --fold=2 --gpu-mem=0.3 "${program_args[@]}" || handle_error &
thread_list+=("$!")

wait
echo "Training network: done"

# Cleanup
echo "Deleting workspace: started"
cleanup
echo "Deleting workspace: done"
